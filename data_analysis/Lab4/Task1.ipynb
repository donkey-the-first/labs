{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання щодо генерації текстів або машинного перекладу (на вибір) на базі рекурентних мереж або трансформерів (на вибір).  \n",
    "Вирішіть завдання щодо генерації текстів або машинного перекладу. Особливо вітаються україномовні моделі.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Було обрано завдання машиного перекладу на базі рекурентних мереж  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 14:12:08.243637: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 430833664 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/78\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:18\u001b[0m 14s/step - accuracy: 0.0000e+00 - loss: 8.7908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 14:12:09.861335: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 430833664 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 2/78\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 1s/step - accuracy: 0.0283 - loss: 8.7901      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 14:12:11.102209: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 430833664 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 3/78\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 1s/step - accuracy: 0.0620 - loss: 8.7893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 14:12:12.208965: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 430833664 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 4/78\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 1s/step - accuracy: 0.1127 - loss: 8.7885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 14:12:13.341561: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 430833664 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.4417 - loss: 7.3255\n",
      "Epoch 2/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.0762 - loss: 5.0536\n",
      "Epoch 3/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.0804 - loss: 4.7480\n",
      "Epoch 4/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - accuracy: 0.0821 - loss: 4.5357\n",
      "Epoch 5/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - accuracy: 0.0835 - loss: 4.3723\n",
      "Epoch 6/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.0886 - loss: 4.2154\n",
      "Epoch 7/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.0925 - loss: 4.0600\n",
      "Epoch 8/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.0949 - loss: 3.9052\n",
      "Epoch 9/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.0968 - loss: 3.7615\n",
      "Epoch 10/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.0989 - loss: 3.6281\n",
      "Input: Hi.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Translated: це це дуже я я я я я я я я я не я я я я не я я я я не я я я я не я я я\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Обробка датасету\n",
    "file_path = './ukr-eng/ukr.txt'  # Вкажіть шлях до вашого датасету\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Виділення речень\n",
    "english_sentences = []\n",
    "ukrainian_sentences = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) >= 2:\n",
    "        english_sentences.append(parts[0].strip())  # Англійське речення\n",
    "        ukrainian_sentences.append(parts[1].strip())  # Українське речення\n",
    "\n",
    "# Видалення дублювань\n",
    "unique_pairs = list(set(zip(english_sentences, ukrainian_sentences)))\n",
    "english_sentences, ukrainian_sentences = zip(*unique_pairs)\n",
    "\n",
    "# 2. Лімітуємо кількість даних\n",
    "data_limit = 5000  # Максимальна кількість пар для використання\n",
    "english_sentences = english_sentences[:data_limit]\n",
    "ukrainian_sentences = ukrainian_sentences[:data_limit]\n",
    "\n",
    "# Лімітуємо словник\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "# Додаємо спеціальні токени до речень\n",
    "start_token = '<s>'\n",
    "end_token = '<e>'\n",
    "ukrainian_sentences = [f\"{start_token} {sentence} {end_token}\" for sentence in ukrainian_sentences]\n",
    "\n",
    "tokenizer_eng = Tokenizer(num_words=MAX_NUM_WORDS, filters='', lower=True)\n",
    "tokenizer_ukr = Tokenizer(num_words=MAX_NUM_WORDS, filters='', lower=True)\n",
    "\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "tokenizer_ukr.fit_on_texts(ukrainian_sentences)\n",
    "\n",
    "input_sequences = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "target_sequences = tokenizer_ukr.texts_to_sequences(ukrainian_sentences)\n",
    "\n",
    "# Лімітуємо довжину речень\n",
    "MAX_SEQ_LENGTH = 30\n",
    "\n",
    "filtered_input_sequences = []\n",
    "filtered_target_sequences = []\n",
    "\n",
    "for input_seq, target_seq in zip(input_sequences, target_sequences):\n",
    "    if len(input_seq) <= MAX_SEQ_LENGTH and len(target_seq) <= MAX_SEQ_LENGTH:\n",
    "        filtered_input_sequences.append(input_seq)\n",
    "        filtered_target_sequences.append(target_seq)\n",
    "\n",
    "input_sequences = filtered_input_sequences\n",
    "target_sequences = filtered_target_sequences\n",
    "\n",
    "# Паддінг\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "decoder_input_data = pad_sequences(target_sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "\n",
    "# Визначаємо розмір словників\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_ukr = len(tokenizer_ukr.word_index) + 1\n",
    "\n",
    "# 3. Створення моделі Seq2Seq\n",
    "LATENT_DIM = 256\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "# Енкодер\n",
    "encoder_inputs = Input(shape=(MAX_SEQ_LENGTH,), name=\"encoder_input\")\n",
    "encoder_embedding = Embedding(vocab_size_eng, EMBEDDING_DIM, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(LATENT_DIM, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Декодер\n",
    "decoder_inputs = Input(shape=(MAX_SEQ_LENGTH,), name=\"decoder_input\")\n",
    "decoder_embedding = Embedding(vocab_size_ukr, EMBEDDING_DIM, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm, _, _ = LSTM(LATENT_DIM, return_sequences=True, return_state=True)(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_ukr, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_lstm)\n",
    "\n",
    "# Повна модель\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Генератор даних\n",
    "output_signature = (\n",
    "    {\n",
    "        \"encoder_input\": tf.TensorSpec(shape=(None, MAX_SEQ_LENGTH), dtype=tf.int32),\n",
    "        \"decoder_input\": tf.TensorSpec(shape=(None, MAX_SEQ_LENGTH), dtype=tf.int32)\n",
    "    },\n",
    "    tf.TensorSpec(shape=(None, MAX_SEQ_LENGTH, vocab_size_ukr), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "def data_generator(input_data, target_data, batch_size):\n",
    "    for i in range(0, len(input_data), batch_size):\n",
    "        encoder_input_batch = input_data[i:i + batch_size]\n",
    "        decoder_input_batch = target_data[i:i + batch_size]\n",
    "\n",
    "        if len(encoder_input_batch) < batch_size:\n",
    "            break  # уникаємо незаповнених батчів\n",
    "\n",
    "        decoder_target_batch = np.zeros((len(decoder_input_batch), MAX_SEQ_LENGTH, vocab_size_ukr), dtype='float32')\n",
    "        for j, seq in enumerate(decoder_input_batch):\n",
    "            for t in range(1, len(seq)):\n",
    "                decoder_target_batch[j, t - 1, seq[t]] = 1.0\n",
    "\n",
    "        yield {\n",
    "            \"encoder_input\": encoder_input_batch,\n",
    "            \"decoder_input\": decoder_input_batch\n",
    "        }, decoder_target_batch\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(encoder_input_data, decoder_input_data, BATCH_SIZE),\n",
    "    output_signature=output_signature\n",
    ").repeat()\n",
    "\n",
    "# 5. Навчання моделі\n",
    "STEPS_PER_EPOCH = max(1, len(encoder_input_data) // BATCH_SIZE)\n",
    "\n",
    "model.fit(\n",
    "    dataset,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 6. Інференс (переклад)\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_outputs, state_h, state_c = LSTM(LATENT_DIM, return_sequences=True, return_state=True)(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def translate_sentence(input_sentence):\n",
    "    input_seq = tokenizer_eng.texts_to_sequences([input_sentence])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer_ukr.word_index[start_token]\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer_ukr.index_word.get(sampled_token_index, '')\n",
    "        if sampled_word == end_token or len(decoded_sentence.split()) > MAX_SEQ_LENGTH:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "# 7. Тестування перекладу\n",
    "test_sentence = \"Hi.\"\n",
    "print(\"Input:\", test_sentence)\n",
    "print(\"Translated:\", translate_sentence(test_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
